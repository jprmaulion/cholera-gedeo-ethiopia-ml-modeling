{"cells":[{"source":"## Integration of Spatial Patterns and Machine Learning for Cholera Risk Assessment in Southern Ethiopia (PART 2)","metadata":{},"id":"8631cbf2-5cf0-41c4-9a50-e3d7790ba640","cell_type":"markdown"},{"source":"### Abstract\n\nThis notebook builds upon the exploratory analysis of cholera case data from the Gedeo Zone, Ethiopia, during 2023, advancing towards predictive modeling of cholera severity based on clinical and epidemiological features. Leveraging individual-level data enriched by prior feature engineering, this phase applies machine learning techniques to develop, tune, and validate models that classify cholera severity, primarily informed by dehydration status. By integrating model selection, hyperparameter optimization, and performance evaluation, this notebook aims to deliver robust predictive tools to assist healthcare providers in the early identification of severe cases. These models have the potential to enhance clinical decision-making, improve patient outcomes, and optimize resource allocation in cholera-affected areas. The study concludes with an evaluation of the model interpretability, which aligns with the foundational insights established in the preceding exploratory data analysis notebook. Note that I will head straight to the model development.","metadata":{},"id":"1126ecff-9012-4841-b460-446419e3cf5c","cell_type":"markdown"},{"source":"## Data Description\n\nIn this notebook, I will use the individual-level cholera records that captured the cholera outbreak in Gedeo Zone, Ethiopia, during 2023. This dataset was sourced from [[1]](https://data.mendeley.com/datasets/7zz3tp5kt5/1).\n\n---\n\n### CHOLERA1: Individual-level cholera case records\n\nThis dataset contains detailed case-based information for cholera patients, including:\n\n| Column                       | Description / Use                                                                                      |\n| ---------------------------- | ---------------------------------------------------------------------------------------------------- |\n| `SerialNo`                   | Unique identifier for each patient record                                                            |\n| `AgeYear`                   | Patient age in years                                                                                   |\n| `Kebelename`                | Name of kebele (the smallest administrative unit in Ethiopia) where the patient resides               |\n| `HealthFacilityCTC`         | Health facility where case was seen                                                                   |\n| `DateseenathealthfacilityM` | Date patient was seen/admitted at the health facility                                                 |\n| `DateofonsetofdiseaseMMD`   | Date of symptom onset                                                                                 |\n| `Dischargedate`             | Date of discharge from care                                                                           |\n| `Dehaydration_status`       | Dehydration severity (none, some, severe) – key clinical indicator                                    |\n| `Hospitalized`              | Whether patient was hospitalized (yes/no)                                                             |\n| `sex`                      | Patient sex                                                                                          |\n| `Region`, `Zone`, `District`| Administrative divisions for geographic context                                                     |\n| `symptom`, `diarrhoea`, `vomit` | Presence of key symptoms                                                                          |\n| `Occupation`                | Patient occupation                                                                                    |\n| `water`, `Water_source`     | Source of drinking water                                                                              |\n| `Contact_history`, `Travel_history` | Risk factor indicators: recent contact or travel                                             |\n| `outcome`                  | Case outcome status (recovered, died, referred, etc.)                                                 |\n| `Vaccination`              | Cholera vaccination status (yes/no/unknown)                                                          |\n| `Latrine_avalability`      | Whether latrine was available at patient’s household                                                  |\n\n### Data structure takeaways\n\n\n- CHOLERA1 requires cleaning for text standardization and missing values.\n- Temporal data present in CHOLERA1 (dates of onset, admission, discharge) allow temporal trend analysis.\n- Clinical variables such as `Dehaydration_status` are critical for disease severity classification.\n- Water and sanitation variables provide insight into possible environmental drivers and risk exposures.\n- Some column names and elements may have been misspelled (e.g., `Dehaydration_status`), but these were retained in this notebook.\n","metadata":{},"id":"476b368b-131f-43be-a2a5-d09e84b6efcc","cell_type":"markdown"},{"source":"In building the model, I limited the data fields to the following: `Dehaydration_status`,\n`DateofonsetofdiseaseMMD`, `water`, `Latrine_avalability`, `DateseenathealthfacilityM`. Here, I will transform `Dehaydration_status` into a cholera severity metric (also the target binary variable). Columns `DateofonsetofdiseaseMMD` and `DateseenathealthfacilityM` will be converted into a variable that I call onset-to-admission interval (which will then be further cleansed and clipped); this determines the number of days between when the symptom started and when the patient sought care. Finally, `water` and `Latrine_avalability` will be respectively transformed into one-hot encoded and single binary numeric features.","metadata":{},"cell_type":"markdown","id":"ed589b73-a226-4a0e-b8af-06f0dc8e0329"},{"source":"# LOAD DATASETS\n\nimport pandas as pd\nimport numpy as np\nfname = 'Final data.csv'\ncols_needed = ['Dehaydration_status','DateofonsetofdiseaseMMD', 'water', 'Latrine_avalability', 'DateseenathealthfacilityM']\ndf_cholera = pd.read_csv(fname, usecols=cols_needed)","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1761916000045,"lastExecutedByKernel":"8ba82cc2-98e0-453c-8ee8-a0d4af9abef8","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# LOAD DATASETS\n\nimport pandas as pd\nimport numpy as np\nfname = 'Final data.csv'\ncols_needed = ['Dehaydration_status','DateofonsetofdiseaseMMD', 'water', 'Latrine_avalability', 'DateseenathealthfacilityM']\ndf_cholera = pd.read_csv(fname, usecols=cols_needed)","outputsMetadata":{"0":{"height":206,"type":"stream"}}},"id":"8b0ba5e2-6187-4bf9-9564-a40a7f4c7248","cell_type":"code","execution_count":18,"outputs":[]},{"source":"# Display the unique elements per column in df_cholera\nunique_elements = {col: df_cholera[col].unique() for col in df_cholera.columns}\nprint(unique_elements)","metadata":{"executionCancelledAt":null,"executionTime":56,"lastExecutedAt":1761916000102,"lastExecutedByKernel":"8ba82cc2-98e0-453c-8ee8-a0d4af9abef8","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Display the unique elements per column in df_cholera\nunique_elements = {col: df_cholera[col].unique() for col in df_cholera.columns}\nprint(unique_elements)","outputsMetadata":{"0":{"height":567,"type":"stream"}}},"id":"69de248a-a9ac-4e06-baff-c13fb21e7654","cell_type":"code","execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":"{'DateseenathealthfacilityM': array([' 7/30/2023', '  9/8/2023', ' 8/14/2023', '  6/9/2023',\n       ' 5/12/2023', ' 9/21/2023', ' 5/13/2023', '  8/5/2023',\n       '  7/9/2023', ' 8/13/2023', '10/19/2023', ' 9/20/2023',\n       ' 9/17/2023', ' 5/25/2023', '10/25/2023', ' 10/9/2023',\n       ' 9/15/2023', ' 9/23/2023', ' 8/17/2023', ' 9/26/2023',\n       ' 7/17/2023', ' 10/5/2023', ' 6/10/2023', ' 5/17/2023',\n       '11/12/2023', ' 9/22/2023', ' 7/11/2023', ' 7/23/2023',\n       ' 9/24/2023', ' 7/28/2023', '  7/8/2023', '  8/1/2023',\n       '10/20/2023', ' 4/22/2023', '10/16/2023', ' 11/2/2023',\n       ' 7/27/2023', ' 9/25/2023', ' 10/2/2023', ' 7/20/2023',\n       ' 9/29/2023', ' 7/24/2023', ' 7/13/2023', ' 4/21/2023',\n       ' 9/28/2023', ' 5/28/2023', ' 7/14/2023', ' 7/19/2023',\n       ' 8/18/2023', ' 5/21/2023', '11/10/2023', ' 7/29/2023',\n       '  8/6/2023', ' 6/27/2023', ' 8/16/2023', ' 4/29/2023',\n       '10/14/2023', ' 8/11/2023', '  7/7/2023', '10/28/2023',\n       '  8/2/2023', ' 10/8/2023', '  8/3/2023', ' 11/9/2023',\n       '10/11/2023', ' 8/19/2023', ' 6/11/2023', ' 8/12/2023',\n       '10/15/2023', ' 9/16/2023', '  6/4/2023', ' 4/20/2023',\n       '  5/8/2023', ' 7/31/2023', '  8/9/2023', '10/22/2023',\n       ' 10/7/2023', ' 9/18/2023', '  5/9/2023', ' 7/22/2023',\n       '  9/6/2023', '  5/3/2023', ' 4/28/2023', '10/13/2023',\n       '10/21/2023', ' 5/10/2023', 'Not seen at HF', ' 5/15/2023',\n       ' 11/5/2023', ' 4/18/2023', ' 7/12/2023', '  6/6/2023',\n       ' 8/15/2023', ' 5/23/2023', ' 7/16/2023', ' 6/13/2023',\n       ' 9/12/2023', ' 7/25/2023', ' 5/18/2023', nan, ' 4/30/2023',\n       ' 8/29/2023', ' 7/26/2023', ' 9/27/2023', '10/10/2023',\n       '10/17/2023', '10/23/2023', ' 10/6/2023', ' 5/22/2023',\n       '  5/5/2023', ' 9/19/2023', '10/31/2023', '  8/4/2023',\n       ' 11/4/2023', '10/30/2023', ' 4/10/2023', '  8/8/2023',\n       '  5/7/2023', ' 5/16/2023', '  6/7/2023', ' 5/24/2023',\n       ' 4/19/2023', '  5/2/2023', '11/19/2023', ' 10/4/2023',\n       '  5/6/2023', '  9/9/2023', '  5/4/2023', ' 10/3/2023',\n       ' 6/25/2023', ' 8/23/2023', ' 9/30/2023', '  6/1/2023',\n       ' 6/12/2023', '10/27/2023', ' 6/14/2023', ' 11/8/2023',\n       ' 5/30/2023', ' 6/18/2023', ' 8/10/2023', ' 5/31/2023',\n       ' 7/10/2023', ' 4/27/2023', '  8/7/2023', ' 6/22/2023',\n       ' 9/10/2023', '11/11/2023', ' 7/21/2023', ' 11/3/2023',\n       ' 8/27/2023', '10/12/2023', ' 8/26/2023', ' 4/13/2023',\n       ' 5/14/2023', ' 11/1/2023', ' 7/18/2023', ' 4/24/2023',\n       '10/24/2023', '11/16/2023', ' 8/30/2023', ' 8/20/2023',\n       ' 5/19/2023', '10/26/2023', ' 11/6/2023', ' 5/29/2023',\n       ' 4/15/2023', ' 4/26/2023', ' 5/20/2023', '  6/8/2023',\n       ' 8/21/2023', '  6/2/2023', '  9/3/2023', '  7/4/2023',\n       '11/15/2023', ' 4/25/2023', ' 4/23/2023', '  7/6/2023',\n       ' 9/14/2023', '  6/3/2023', ' 10/1/2023', '  6/5/2023',\n       ' 5/27/2023', ' 4/14/2023'], dtype=object), 'DateofonsetofdiseaseMMD': array([' 7/30/2023', '  9/8/2023', ' 8/14/2023', '  6/5/2023',\n       ' 5/11/2023', ' 9/21/2023', ' 5/12/2023', '  8/5/2023',\n       '  7/9/2023', ' 8/13/2023', '10/19/2023', ' 9/19/2023',\n       ' 9/16/2023', ' 5/24/2023', '10/25/2023', ' 10/9/2023',\n       ' 9/14/2023', ' 9/22/2023', ' 8/17/2023', ' 9/26/2023',\n       ' 7/16/2023', ' 10/4/2023', ' 6/10/2023', ' 5/17/2023',\n       '11/12/2023', ' 9/20/2023', ' 7/11/2023', ' 7/23/2023',\n       ' 9/24/2023', ' 7/27/2023', '  7/8/2023', '  8/1/2023',\n       '10/20/2023', ' 4/21/2023', '10/16/2023', ' 11/2/2023',\n       ' 10/2/2023', ' 7/20/2023', ' 9/28/2023', ' 7/24/2023',\n       ' 7/13/2023', ' 4/20/2023', ' 7/10/2023', ' 5/27/2023',\n       ' 7/18/2023', ' 5/19/2023', '11/10/2023', ' 7/28/2023',\n       ' 6/26/2023', ' 9/17/2023', ' 8/16/2023', ' 7/29/2023',\n       ' 4/27/2023', '10/14/2023', ' 5/16/2023', ' 8/11/2023',\n       ' 9/25/2023', '  7/6/2023', '10/28/2023', ' 10/5/2023',\n       '  8/2/2023', '  8/3/2023', ' 11/9/2023', '10/11/2023',\n       ' 8/18/2023', ' 8/12/2023', '10/15/2023', '  7/7/2023',\n       '  6/4/2023', '  5/7/2023', '  8/9/2023', '  8/6/2023',\n       '10/22/2023', ' 10/6/2023', '  5/8/2023', '  9/6/2023',\n       '  4/2/2023', ' 4/25/2023', '10/13/2023', '10/21/2023',\n       ' 5/10/2023', nan, ' 5/15/2023', ' 11/4/2023', ' 9/18/2023',\n       ' 4/17/2023', ' 7/12/2023', '  6/6/2023', ' 10/8/2023',\n       ' 5/23/2023', ' 6/12/2023', ' 9/12/2023', ' 5/18/2023',\n       ' 8/10/2023', ' 4/28/2023', ' 8/28/2023', ' 7/26/2023',\n       '10/10/2023', '10/17/2023', ' 7/15/2023', ' 4/19/2023',\n       '10/23/2023', ' 5/22/2023', '  5/5/2023', '10/31/2023',\n       '  8/4/2023', ' 9/23/2023', '10/30/2023', '  4/9/2023',\n       '  8/8/2023', '  5/3/2023', ' 7/21/2023', ' 8/15/2023',\n       '  5/1/2023', '11/18/2023', '  6/3/2023', '  5/6/2023',\n       ' 5/20/2023', '  9/9/2023', ' 6/24/2023', ' 8/22/2023',\n       ' 8/19/2023', ' 9/30/2023', ' 5/31/2023', '10/27/2023',\n       ' 6/14/2023', ' 11/8/2023', ' 7/22/2023', '  5/2/2023',\n       ' 4/18/2023', ' 5/30/2023', ' 6/17/2023', '  5/9/2023',\n       ' 5/29/2023', ' 7/19/2023', ' 5/14/2023', ' 9/15/2023',\n       ' 6/21/2023', ' 9/10/2023', '11/11/2023', '  8/7/2023',\n       ' 9/27/2023', ' 11/3/2023', ' 5/28/2023', ' 4/29/2023',\n       '10/12/2023', ' 8/26/2023', ' 8/25/2023', ' 4/12/2023',\n       ' 9/29/2023', ' 11/1/2023', ' 5/21/2023', ' 4/23/2023',\n       '10/24/2023', ' 7/31/2023', ' 4/22/2023', '11/15/2023',\n       ' 10/1/2023', ' 8/30/2023', ' 8/20/2023', ' 5/13/2023',\n       '  6/1/2023', '10/26/2023', ' 8/29/2023', ' 11/6/2023',\n       ' 4/14/2023', ' 10/7/2023', '  5/4/2023', '  6/7/2023',\n       ' 8/21/2023', '  6/2/2023', '  9/3/2023', '  7/1/2023',\n       '  6/9/2023', ' 7/25/2023', ' 6/11/2023', ' 6/18/2023',\n       ' 10/3/2023', ' 6/13/2023', '  6/8/2023'], dtype=object), 'Dehaydration_status': array(['severe dehaydration', 'some dehaydration', 'no dehaydration',\n       'Unknown'], dtype=object), 'water': array(['Tap water', 'river', 'River', 'Protected spiring',\n       'Unprotected Spring', 'Hand Pump', nan, 'Hand Dug Well',\n       'Bore Hole Water', 'Rain Water', 'RIVER'], dtype=object), 'Latrine_avalability': array(['Yes', 'No', nan], dtype=object)}\n"}]},{"source":"# Cleaning includes:\n# - Date parsing and cleaning\n# - String standardization (lowercase, strip)\n# - Unification of categories (for `water`)\n# - Missing value handling\n# - Creating a binary target variable called `cholera_severe` based on severe dehydration status \n\n# ========== Step 1: Clean Date Columns ==========\n# Strip and convert date columns to datetime, coercing errors to NaT\nfor col in ['DateseenathealthfacilityM']:\n    df_cholera[col] = pd.to_datetime(df_cholera[col].astype(str).str.strip(), errors='coerce')\n\n# Confirm existing DateofonsetofdiseaseMMD is datetime type, if not convert\nif not pd.api.types.is_datetime64_any_dtype(df_cholera['DateofonsetofdiseaseMMD']):\n    df_cholera['DateofonsetofdiseaseMMD'] = pd.to_datetime(df_cholera['DateofonsetofdiseaseMMD'], errors='coerce')\n\n# ========== Step 2: Standardize Text Columns ==========\ntext_cols = ['water', 'Latrine_avalability']\n\nfor col in text_cols:\n    df_cholera[col] = df_cholera[col].astype(str).str.lower().str.strip()\n\n# ========== Step 3: Unify Water Source Categories ==========\n# Helper function to unify water categories\ndef unify_water_source(x):\n    if pd.isna(x) or x == 'nan' or x == 'unknown':\n        return 'unknown'\n    x = x.lower().strip()\n    if 'river' in x:\n        return 'river'\n    elif 'protected spring' in x or 'protected spiring' in x:\n        return 'protected spring'\n    elif 'unprotected spring' in x:\n        return 'unprotected spring'\n    elif 'hand pump' in x:\n        return 'hand pump'\n    elif 'hand dug well' in x:\n        return 'hand dug well'\n    elif 'bore hole' in x:\n        return 'bore hole water'\n    elif 'rain water' in x:\n        return 'rain water'\n    elif 'tap water' in x:\n        return 'tap water'\n    else:\n        return x\n\n# Apply to both water columns\ndf_cholera['water'] = df_cholera['water'].apply(unify_water_source)\n\n# ========== Step 4: Handle Missing Values for Categorical Columns ==========\nfill_unknown_cols = ['Latrine_avalability']\n\nfor col in fill_unknown_cols:\n    df_cholera[col] = df_cholera[col].replace(['nan', np.nan, 'none', ''], 'unknown')\n\n# ========== Step 5: Create Target Variable ==========\n# Example: binary for severe dehydration (1 if severe, else 0)\ndf_cholera['cholera_severe'] = df_cholera['Dehaydration_status'].apply(lambda x: 1 if 'severe' in str(x) else 0)\n\n\n# ========== Step 6: Onset-to-admission interval ==========\ndf_cholera['days_onset_to_admission'] = (\n    df_cholera['DateseenathealthfacilityM'] - df_cholera['DateofonsetofdiseaseMMD']\n).dt.days\n\ndf_cholera['days_onset_to_admission'].value_counts()\n\n\n\n# Identify rows where time difference is negative\nneg_diff_mask = df_cholera['days_onset_to_admission'] < 0\n\nprint(f\"Number of rows with negative intervals: {neg_diff_mask.sum()}\")\n\n# Drop these rows\ndf_cholera = df_cholera.loc[~neg_diff_mask]\n\nprint(f\"Rows after dropping: {df_cholera.shape[0]}\")\n\n\n# Cap values at 30\ndf_cholera['days_onset_to_admission'] = df_cholera['days_onset_to_admission'].clip(upper=30)\n\ndf_cholera = df_cholera[df_cholera['days_onset_to_admission'].notnull() & (df_cholera['days_onset_to_admission'] >= 0)].copy()\n\n# ========== Step 7: Final Check ==========\nprint(\"Sample cleaned data:\")\nprint(df_cholera.head())\n\n# Check unique counts of key columns after cleaning\nfor col in ['water', 'Latrine_avalability', 'Dehaydration_status', 'cholera_severe', 'days_onset_to_admission']:\n    print(f\"Unique values in {col}: {df_cholera[col].unique()}\")","metadata":{"executionCancelledAt":null,"executionTime":64,"lastExecutedAt":1761916000166,"lastExecutedByKernel":"8ba82cc2-98e0-453c-8ee8-a0d4af9abef8","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Cleaning includes:\n# - Date parsing and cleaning\n# - String standardization (lowercase, strip)\n# - Unification of categories (for `water`)\n# - Missing value handling\n# - Creating a binary target variable called `cholera_severe` based on severe dehydration status \n\n# ========== Step 1: Clean Date Columns ==========\n# Strip and convert date columns to datetime, coercing errors to NaT\nfor col in ['DateseenathealthfacilityM']:\n    df_cholera[col] = pd.to_datetime(df_cholera[col].astype(str).str.strip(), errors='coerce')\n\n# Confirm existing DateofonsetofdiseaseMMD is datetime type, if not convert\nif not pd.api.types.is_datetime64_any_dtype(df_cholera['DateofonsetofdiseaseMMD']):\n    df_cholera['DateofonsetofdiseaseMMD'] = pd.to_datetime(df_cholera['DateofonsetofdiseaseMMD'], errors='coerce')\n\n# ========== Step 2: Standardize Text Columns ==========\ntext_cols = ['water', 'Latrine_avalability']\n\nfor col in text_cols:\n    df_cholera[col] = df_cholera[col].astype(str).str.lower().str.strip()\n\n# ========== Step 3: Unify Water Source Categories ==========\n# Helper function to unify water categories\ndef unify_water_source(x):\n    if pd.isna(x) or x == 'nan' or x == 'unknown':\n        return 'unknown'\n    x = x.lower().strip()\n    if 'river' in x:\n        return 'river'\n    elif 'protected spring' in x or 'protected spiring' in x:\n        return 'protected spring'\n    elif 'unprotected spring' in x:\n        return 'unprotected spring'\n    elif 'hand pump' in x:\n        return 'hand pump'\n    elif 'hand dug well' in x:\n        return 'hand dug well'\n    elif 'bore hole' in x:\n        return 'bore hole water'\n    elif 'rain water' in x:\n        return 'rain water'\n    elif 'tap water' in x:\n        return 'tap water'\n    else:\n        return x\n\n# Apply to both water columns\ndf_cholera['water'] = df_cholera['water'].apply(unify_water_source)\n\n# ========== Step 4: Handle Missing Values for Categorical Columns ==========\nfill_unknown_cols = ['Latrine_avalability']\n\nfor col in fill_unknown_cols:\n    df_cholera[col] = df_cholera[col].replace(['nan', np.nan, 'none', ''], 'unknown')\n\n# ========== Step 5: Create Target Variable ==========\n# Example: binary for severe dehydration (1 if severe, else 0)\ndf_cholera['cholera_severe'] = df_cholera['Dehaydration_status'].apply(lambda x: 1 if 'severe' in str(x) else 0)\n\n\n# ========== Step 6: Onset-to-admission interval ==========\ndf_cholera['days_onset_to_admission'] = (\n    df_cholera['DateseenathealthfacilityM'] - df_cholera['DateofonsetofdiseaseMMD']\n).dt.days\n\ndf_cholera['days_onset_to_admission'].value_counts()\n\n\n\n# Identify rows where time difference is negative\nneg_diff_mask = df_cholera['days_onset_to_admission'] < 0\n\nprint(f\"Number of rows with negative intervals: {neg_diff_mask.sum()}\")\n\n# Drop these rows\ndf_cholera = df_cholera.loc[~neg_diff_mask]\n\nprint(f\"Rows after dropping: {df_cholera.shape[0]}\")\n\n\n# Cap values at 30\ndf_cholera['days_onset_to_admission'] = df_cholera['days_onset_to_admission'].clip(upper=30)\n\ndf_cholera = df_cholera[df_cholera['days_onset_to_admission'].notnull() & (df_cholera['days_onset_to_admission'] >= 0)].copy()\n\n# ========== Step 7: Final Check ==========\nprint(\"Sample cleaned data:\")\nprint(df_cholera.head())\n\n# Check unique counts of key columns after cleaning\nfor col in ['water', 'Latrine_avalability', 'Dehaydration_status', 'cholera_severe', 'days_onset_to_admission']:\n    print(f\"Unique values in {col}: {df_cholera[col].unique()}\")","outputsMetadata":{"0":{"height":395,"type":"stream"}}},"id":"075420ea-6a14-4780-8e0f-c508d0b091bd","cell_type":"code","execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":"Number of rows with negative intervals: 3\nRows after dropping: 789\nSample cleaned data:\n  DateseenathealthfacilityM  ... days_onset_to_admission\n0                2023-07-30  ...                     0.0\n1                2023-09-08  ...                     0.0\n2                2023-08-14  ...                     0.0\n3                2023-06-09  ...                     4.0\n4                2023-05-12  ...                     1.0\n\n[5 rows x 7 columns]\nUnique values in water: ['tap water' 'river' 'protected spring' 'hand pump' 'unknown'\n 'hand dug well' 'bore hole water']\nUnique values in Latrine_avalability: ['yes' 'no' 'unknown']\nUnique values in Dehaydration_status: ['severe dehaydration' 'some dehaydration' 'no dehaydration' 'Unknown']\nUnique values in cholera_severe: [1 0]\nUnique values in days_onset_to_admission: [ 0.  4.  1.  2. 30.  3.  5.]\n"}]},{"source":"# Modeling phase\n\n\nThe next code block now implements the full machine learning pipeline to predict cholera severity using epidemiological and clinical data. After preprocessing the primary dataset, it is then split into stratified training and testing subsets to maintain class proportions. A preprocessing pipeline is defined using `ColumnTransformer` to standardize the numeric onset-to-admission interval while passing one-hot encoded categorical features unchanged. To address class imbalance, the scale factor `scale_pos_weight` is computed for use by **XGBoost**. The code performs **Bayesian hyperparameter optimization** with 5-fold cross-validation on the preprocessed training set using the XGBoost `DMatrix` format, tuning parameters such as tree depth, gamma, subsample, and learning rate for improved ROC-AUC. After identifying the best hyperparameters, a full pipeline combining preprocessing and the tuned XGBoost model is trained with early stopping evaluated on the preprocessed test set to prevent overfitting. Model performance is assessed via classification metrics and ROC-AUC on unseen test data. For comparison, a similarly preprocessed Random Forest classifier is trained and evaluated as a baseline. The entire pipeline highlights robust feature engineering, data preprocessing, hyperparameter tuning, and model evaluation to build an effective predictive system for cholera severity classification.","metadata":{},"cell_type":"markdown","id":"3c0ba792-58ca-4de4-a416-75be7a3b50ae"},{"source":"!pip install bayesian-optimization\n!pip install --upgrade xgboost","metadata":{"executionCancelledAt":null,"executionTime":3375,"lastExecutedAt":1761916003541,"lastExecutedByKernel":"8ba82cc2-98e0-453c-8ee8-a0d4af9abef8","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install bayesian-optimization\n!pip install --upgrade xgboost","outputsMetadata":{"0":{"height":458,"type":"stream"}}},"cell_type":"code","id":"c8298287-28b4-4090-96ac-de5b2d06ce3f","outputs":[{"output_type":"stream","name":"stdout","text":"Requirement already satisfied: bayesian-optimization in /home/repl/.venv-user/lib/python3.12/site-packages (3.1.0)\nRequirement already satisfied: colorama>=0.4.6 in /home/repl/.venv-user/lib/python3.12/site-packages (from bayesian-optimization) (0.4.6)\nRequirement already satisfied: numpy>=1.25 in /home/repl/.venv-user/lib/python3.12/site-packages (from bayesian-optimization) (2.1.3)\nRequirement already satisfied: scikit-learn>=1.0.0 in /home/repl/.venv-user/lib/python3.12/site-packages (from bayesian-optimization) (1.7.1)\nRequirement already satisfied: scipy>=1.0.0 in /home/repl/.venv-user/lib/python3.12/site-packages (from bayesian-optimization) (1.15.3)\nRequirement already satisfied: joblib>=1.2.0 in /home/repl/.venv-user/lib/python3.12/site-packages (from scikit-learn>=1.0.0->bayesian-optimization) (1.5.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /home/repl/.venv-user/lib/python3.12/site-packages (from scikit-learn>=1.0.0->bayesian-optimization) (3.6.0)\nRequirement already satisfied: xgboost in /home/repl/.venv-user/lib/python3.12/site-packages (3.1.1)\nRequirement already satisfied: numpy in /home/repl/.venv-user/lib/python3.12/site-packages (from xgboost) (2.1.3)\nRequirement already satisfied: nvidia-nccl-cu12 in /home/repl/.venv-user/lib/python3.12/site-packages (from xgboost) (2.26.2)\nRequirement already satisfied: scipy in /home/repl/.venv-user/lib/python3.12/site-packages (from xgboost) (1.15.3)\n"}],"execution_count":21},{"source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score, classification_report, accuracy_score, confusion_matrix\n\nfrom xgboost import XGBClassifier, DMatrix, cv\nfrom bayes_opt import BayesianOptimization\nimport xgboost as xgb\n\n# ========== STEP 1: Define Features and Target with 'days_onset_to_admission' ==========\n\ndef prepare_features_target(df):\n    \"\"\"\n    Prepare features X and target y including:\n    - One-hot encoding of select water sources\n    - Binary encoding of latrine availability\n    - Numeric inclusion of days from onset to admission (capped and cleaned)\n    \"\"\"\n\n    # Target\n    y = df['cholera_severe']\n\n    # Define categorical water sources of interest\n    water_categories = ['tap water', 'river', 'protected spring', 'hand dug well', 'bore hole water']\n\n    # Create 'water_cleaned' categorical to group uncommon waters as 'other'\n    df['water_cleaned'] = df['water'].apply(lambda x: x if x in water_categories else 'other')\n\n    # One-hot encode\n    water_ohe = pd.get_dummies(df['water_cleaned'], prefix='water')\n\n    # Latrine binary encoding (yes=1, else=0)\n    latrine_bin = df['Latrine_avalability'].apply(lambda x: 1 if x == 'yes' else 0).rename('latrine_presence')\n\n    # Combine all features (water dummies + latrine + numeric onset-admission days)\n    X = pd.concat([water_ohe, latrine_bin, df['days_onset_to_admission']], axis=1)\n\n    return X, y\n\n# ========== STEP 2: Split Data Function ==========\n\ndef split_data(X, y, test_size=0.15, random_state=42):\n    return train_test_split(X, y, stratify=y, test_size=test_size, random_state=random_state)\n\n# ========== STEP 3: Evaluation ==========\n\ndef evaluate_model(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n    if hasattr(model, \"predict_proba\"):\n        y_proba = model.predict_proba(X_test)[:,1]\n    elif hasattr(model, \"decision_function\"):\n        y_proba = model.decision_function(X_test)\n    else:\n        y_proba = None\n\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n\n    # Calculate and print accuracy\n    acc = accuracy_score(y_test, y_pred)\n    print(f\"Accuracy: {acc:.4f}\")\n\n    if y_proba is not None:\n        print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n\n    # Print confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix:\")\n    print(cm)\n    \n\n\ndef get_scale_pos_weight(y):\n    num_pos = np.sum(y == 1)\n    num_neg = np.sum(y == 0)\n    return num_neg / num_pos if num_pos > 0 else 1\n\n# ========== STEP 4: Bayesian Optimization CV Function ==========\n\ndef xgb_cv(max_depth, gamma, subsample, colsample_bytree, min_child_weight, learning_rate):\n    params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        'use_label_encoder': False,\n        'tree_method': 'hist',\n        'max_depth': int(max_depth),\n        'gamma': gamma,\n        'subsample': subsample,\n        'colsample_bytree': colsample_bytree,\n        'min_child_weight': int(min_child_weight),\n        'learning_rate': learning_rate,\n        'scale_pos_weight': scale_pos_weight\n    }\n\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=100,\n        nfold=5,\n        stratified=True,\n        early_stopping_rounds=10,\n        seed=42,\n        metrics='auc',\n        verbose_eval=False\n    )\n\n    return cv_results['test-auc-mean'].max()\n\n# ========== PRIMARY SCRIPT ==========\n\n# Check missing columns\nrequired_cols = ['cholera_severe', 'water', 'Latrine_avalability', 'DateseenathealthfacilityM', 'DateofonsetofdiseaseMMD']\nmissing_cols = set(required_cols) - set(df_cholera.columns)\nif missing_cols:\n    raise ValueError(f\"Missing columns in df_cholera needed: {missing_cols}\")\n\nprint(\"Preparing features and target...\")\nX, y = prepare_features_target(df_cholera)\n\nprint(f\"Features shape after preparation: {X.shape}\")\n\n# Split train/test\nX_train, X_test, y_train, y_test = split_data(X, y)\n\nprint(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")\nprint(f\"Train target distribution:\\n{y_train.value_counts(normalize=True)}\")\n\n# Convert any boolean columns to int \nbool_cols_train = X_train.select_dtypes(include='bool').columns\nfor col in bool_cols_train:\n    X_train[col] = X_train[col].astype(int)\nbool_cols_test = X_test.select_dtypes(include='bool').columns\nfor col in bool_cols_test:\n    X_test[col] = X_test[col].astype(int)\n\n# Calculate scale_pos_weight for XGBoost imbalance handling\nscale_pos_weight = get_scale_pos_weight(y_train)\nprint(f\"Calculated scale_pos_weight: {scale_pos_weight:.3f}\")\n\n# Preprocessing pipeline: scale only the numeric 'days_onset_to_admission'\nnumeric_features = ['days_onset_to_admission']\ncategorical_features = [col for col in X.columns if col != 'days_onset_to_admission']\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', StandardScaler(), numeric_features),\n    ('cat', 'passthrough', categorical_features)\n])\n\n# Fit preprocessor on train and transform both train and test for Bayesian Optimization DMatrix\nX_train_preprocessed = preprocessor.fit_transform(X_train)\nX_test_preprocessed = preprocessor.transform(X_test)\n\n\n# Create DMatrix for Bayesian Optimization\ndtrain = DMatrix(X_train_preprocessed, label=y_train)\n\n# ========== Bayesian Optimization of XGBoost Hyperparameters ==========\n\npbounds = {\n    'max_depth': (3, 10),\n    'gamma': (0, 5),\n    'subsample': (0.6, 1),\n    'colsample_bytree': (0.6, 1),\n    'min_child_weight': (1, 10),\n    'learning_rate': (0.01, 0.3)\n}\n\noptimizer = BayesianOptimization(f=xgb_cv, pbounds=pbounds, random_state=42, verbose=2)\n\nprint(\"\\nStarting Bayesian Optimization...\\n\")\noptimizer.maximize(init_points=10, n_iter=25)\n\nbest_params = optimizer.max['params']\nbest_params['max_depth'] = int(best_params['max_depth'])\nbest_params['min_child_weight'] = int(best_params['min_child_weight'])\n\n# Fix remaining params\nbest_params.update({\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'use_label_encoder': False,\n    'tree_method': 'hist',\n    'scale_pos_weight': scale_pos_weight,\n    'seed': 1\n})\n\nprint(\"\\nBest hyperparameters found:\")\nprint(best_params)\n\n# ========== Final Model Training with Pipeline ==========\n\n\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', XGBClassifier(**best_params, n_estimators=1000, early_stopping_rounds=20))\n])\n\n\nX_test_preprocessed = pipeline.named_steps['preprocessor'].transform(X_test)\n\n\n### pass this preprocessed data as the eval_set inside `.fit()`:\n\npipeline.fit(\n    X_train,\n    y_train,\n    classifier__eval_set=[(X_test_preprocessed, y_test)],\n    classifier__verbose=20\n)\n\n\nprint(\"\\nXGBoost Performance on Test Set:\")\nevaluate_model(pipeline, X_test, y_test)\n\n# ========== Random Forest for Comparison ==========\n\nrf_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(\n        n_estimators=500,\n        class_weight='balanced',\n        random_state=42,\n        n_jobs=-1))\n])\n\nrf_pipeline.fit(X_train, y_train)\n\nprint(\"\\nRandom Forest Performance on Test Set:\")\nevaluate_model(rf_pipeline, X_test, y_test)\n\nprint(\"\\nModeling complete.\")\n","metadata":{"executionCancelledAt":null,"executionTime":7470,"lastExecutedAt":1761916011012,"lastExecutedByKernel":"8ba82cc2-98e0-453c-8ee8-a0d4af9abef8","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score, classification_report, accuracy_score, confusion_matrix\n\nfrom xgboost import XGBClassifier, DMatrix, cv\nfrom bayes_opt import BayesianOptimization\nimport xgboost as xgb\n\n# ========== STEP 1: Define Features and Target with 'days_onset_to_admission' ==========\n\ndef prepare_features_target(df):\n    \"\"\"\n    Prepare features X and target y including:\n    - One-hot encoding of select water sources\n    - Binary encoding of latrine availability\n    - Numeric inclusion of days from onset to admission (capped and cleaned)\n    \"\"\"\n\n    # Target\n    y = df['cholera_severe']\n\n    # Define categorical water sources of interest\n    water_categories = ['tap water', 'river', 'protected spring', 'hand dug well', 'bore hole water']\n\n    # Create 'water_cleaned' categorical to group uncommon waters as 'other'\n    df['water_cleaned'] = df['water'].apply(lambda x: x if x in water_categories else 'other')\n\n    # One-hot encode\n    water_ohe = pd.get_dummies(df['water_cleaned'], prefix='water')\n\n    # Latrine binary encoding (yes=1, else=0)\n    latrine_bin = df['Latrine_avalability'].apply(lambda x: 1 if x == 'yes' else 0).rename('latrine_presence')\n\n    # Combine all features (water dummies + latrine + numeric onset-admission days)\n    X = pd.concat([water_ohe, latrine_bin, df['days_onset_to_admission']], axis=1)\n\n    return X, y\n\n# ========== STEP 2: Split Data Function ==========\n\ndef split_data(X, y, test_size=0.15, random_state=42):\n    return train_test_split(X, y, stratify=y, test_size=test_size, random_state=random_state)\n\n# ========== STEP 3: Evaluation ==========\n\ndef evaluate_model(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n    if hasattr(model, \"predict_proba\"):\n        y_proba = model.predict_proba(X_test)[:,1]\n    elif hasattr(model, \"decision_function\"):\n        y_proba = model.decision_function(X_test)\n    else:\n        y_proba = None\n\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n\n    # Calculate and print accuracy\n    acc = accuracy_score(y_test, y_pred)\n    print(f\"Accuracy: {acc:.4f}\")\n\n    if y_proba is not None:\n        print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n\n    # Print confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix:\")\n    print(cm)\n    \n\n\ndef get_scale_pos_weight(y):\n    num_pos = np.sum(y == 1)\n    num_neg = np.sum(y == 0)\n    return num_neg / num_pos if num_pos > 0 else 1\n\n# ========== STEP 4: Bayesian Optimization CV Function ==========\n\ndef xgb_cv(max_depth, gamma, subsample, colsample_bytree, min_child_weight, learning_rate):\n    params = {\n        'objective': 'binary:logistic',\n        'eval_metric': 'auc',\n        'use_label_encoder': False,\n        'tree_method': 'hist',\n        'max_depth': int(max_depth),\n        'gamma': gamma,\n        'subsample': subsample,\n        'colsample_bytree': colsample_bytree,\n        'min_child_weight': int(min_child_weight),\n        'learning_rate': learning_rate,\n        'scale_pos_weight': scale_pos_weight\n    }\n\n    cv_results = xgb.cv(\n        params,\n        dtrain,\n        num_boost_round=100,\n        nfold=5,\n        stratified=True,\n        early_stopping_rounds=10,\n        seed=42,\n        metrics='auc',\n        verbose_eval=False\n    )\n\n    return cv_results['test-auc-mean'].max()\n\n# ========== PRIMARY SCRIPT ==========\n\n# Check missing columns\nrequired_cols = ['cholera_severe', 'water', 'Latrine_avalability', 'DateseenathealthfacilityM', 'DateofonsetofdiseaseMMD']\nmissing_cols = set(required_cols) - set(df_cholera.columns)\nif missing_cols:\n    raise ValueError(f\"Missing columns in df_cholera needed: {missing_cols}\")\n\nprint(\"Preparing features and target...\")\nX, y = prepare_features_target(df_cholera)\n\nprint(f\"Features shape after preparation: {X.shape}\")\n\n# Split train/test\nX_train, X_test, y_train, y_test = split_data(X, y)\n\nprint(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")\nprint(f\"Train target distribution:\\n{y_train.value_counts(normalize=True)}\")\n\n# Convert any boolean columns to int \nbool_cols_train = X_train.select_dtypes(include='bool').columns\nfor col in bool_cols_train:\n    X_train[col] = X_train[col].astype(int)\nbool_cols_test = X_test.select_dtypes(include='bool').columns\nfor col in bool_cols_test:\n    X_test[col] = X_test[col].astype(int)\n\n# Calculate scale_pos_weight for XGBoost imbalance handling\nscale_pos_weight = get_scale_pos_weight(y_train)\nprint(f\"Calculated scale_pos_weight: {scale_pos_weight:.3f}\")\n\n# Preprocessing pipeline: scale only the numeric 'days_onset_to_admission'\nnumeric_features = ['days_onset_to_admission']\ncategorical_features = [col for col in X.columns if col != 'days_onset_to_admission']\n\npreprocessor = ColumnTransformer(transformers=[\n    ('num', StandardScaler(), numeric_features),\n    ('cat', 'passthrough', categorical_features)\n])\n\n# Fit preprocessor on train and transform both train and test for Bayesian Optimization DMatrix\nX_train_preprocessed = preprocessor.fit_transform(X_train)\nX_test_preprocessed = preprocessor.transform(X_test)\n\n\n# Create DMatrix for Bayesian Optimization\ndtrain = DMatrix(X_train_preprocessed, label=y_train)\n\n# ========== Bayesian Optimization of XGBoost Hyperparameters ==========\n\npbounds = {\n    'max_depth': (3, 10),\n    'gamma': (0, 5),\n    'subsample': (0.6, 1),\n    'colsample_bytree': (0.6, 1),\n    'min_child_weight': (1, 10),\n    'learning_rate': (0.01, 0.3)\n}\n\noptimizer = BayesianOptimization(f=xgb_cv, pbounds=pbounds, random_state=42, verbose=2)\n\nprint(\"\\nStarting Bayesian Optimization...\\n\")\noptimizer.maximize(init_points=10, n_iter=25)\n\nbest_params = optimizer.max['params']\nbest_params['max_depth'] = int(best_params['max_depth'])\nbest_params['min_child_weight'] = int(best_params['min_child_weight'])\n\n# Fix remaining params\nbest_params.update({\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'use_label_encoder': False,\n    'tree_method': 'hist',\n    'scale_pos_weight': scale_pos_weight,\n    'seed': 1\n})\n\nprint(\"\\nBest hyperparameters found:\")\nprint(best_params)\n\n# ========== Final Model Training with Pipeline ==========\n\n\n\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', XGBClassifier(**best_params, n_estimators=1000, early_stopping_rounds=20))\n])\n\n\nX_test_preprocessed = pipeline.named_steps['preprocessor'].transform(X_test)\n\n\n### pass this preprocessed data as the eval_set inside `.fit()`:\n\npipeline.fit(\n    X_train,\n    y_train,\n    classifier__eval_set=[(X_test_preprocessed, y_test)],\n    classifier__verbose=20\n)\n\n\nprint(\"\\nXGBoost Performance on Test Set:\")\nevaluate_model(pipeline, X_test, y_test)\n\n# ========== Random Forest for Comparison ==========\n\nrf_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(\n        n_estimators=500,\n        class_weight='balanced',\n        random_state=42,\n        n_jobs=-1))\n])\n\nrf_pipeline.fit(X_train, y_train)\n\nprint(\"\\nRandom Forest Performance on Test Set:\")\nevaluate_model(rf_pipeline, X_test, y_test)\n\nprint(\"\\nModeling complete.\")\n","outputsMetadata":{"0":{"height":567,"type":"stream"},"2":{"height":374,"type":"stream"},"4":{"height":59,"type":"stream"}}},"cell_type":"code","id":"d6ff86e4-6a3a-4d2f-8f1a-dde5221bead8","outputs":[{"output_type":"stream","name":"stdout","text":"Preparing features and target...\nFeatures shape after preparation: (691, 8)\nTrain size: 587, Test size: 104\nTrain target distribution:\ncholera_severe\n0    0.630324\n1    0.369676\nName: proportion, dtype: float64\nCalculated scale_pos_weight: 1.705\n\nStarting Bayesian Optimization...\n\n|   iter    |  target   | max_depth |   gamma   | subsample | colsam... | min_ch... | learni... |\n-------------------------------------------------------------------------------------------------\n| \u001b[39m1        \u001b[39m | \u001b[39m0.5116000\u001b[39m | \u001b[39m5.6217808\u001b[39m | \u001b[39m4.7535715\u001b[39m | \u001b[39m0.8927975\u001b[39m | \u001b[39m0.8394633\u001b[39m | \u001b[39m2.4041677\u001b[39m | \u001b[39m0.0552384\u001b[39m |\n| \u001b[35m2        \u001b[39m | \u001b[35m0.5158776\u001b[39m | \u001b[35m3.4065852\u001b[39m | \u001b[35m4.3308807\u001b[39m | \u001b[35m0.8404460\u001b[39m | \u001b[35m0.8832290\u001b[39m | \u001b[35m1.1852604\u001b[39m | \u001b[35m0.2912738\u001b[39m |\n| \u001b[35m3        \u001b[39m | \u001b[35m0.5448524\u001b[39m | \u001b[35m8.8270984\u001b[39m | \u001b[35m1.0616955\u001b[39m | \u001b[35m0.6727299\u001b[39m | \u001b[35m0.6733618\u001b[39m | \u001b[35m3.7381801\u001b[39m | \u001b[35m0.1621793\u001b[39m |\n| \u001b[39m4        \u001b[39m | \u001b[39m0.5305253\u001b[39m | \u001b[39m6.0236151\u001b[39m | \u001b[39m1.4561457\u001b[39m | \u001b[39m0.8447411\u001b[39m | \u001b[39m0.6557975\u001b[39m | \u001b[39m3.6293018\u001b[39m | \u001b[39m0.1162449\u001b[39m |\n| \u001b[39m5        \u001b[39m | \u001b[39m0.5224772\u001b[39m | \u001b[39m6.1924898\u001b[39m | \u001b[39m3.9258798\u001b[39m | \u001b[39m0.6798695\u001b[39m | \u001b[39m0.8056937\u001b[39m | \u001b[39m6.3317311\u001b[39m | \u001b[39m0.0234706\u001b[39m |\n| \u001b[35m6        \u001b[39m | \u001b[35m0.5474122\u001b[39m | \u001b[35m7.2528139\u001b[39m | \u001b[35m0.8526206\u001b[39m | \u001b[35m0.6260206\u001b[39m | \u001b[35m0.9795542\u001b[39m | \u001b[35m9.6906882\u001b[39m | \u001b[35m0.2444352\u001b[39m |\n| \u001b[39m7        \u001b[39m | \u001b[39m0.5343966\u001b[39m | \u001b[39m5.1322963\u001b[39m | \u001b[39m0.4883605\u001b[39m | \u001b[39m0.8736932\u001b[39m | \u001b[39m0.7760609\u001b[39m | \u001b[39m2.0983441\u001b[39m | \u001b[39m0.1536013\u001b[39m |\n| \u001b[39m8        \u001b[39m | \u001b[39m0.5286783\u001b[39m | \u001b[39m3.2407196\u001b[39m | \u001b[39m4.5466020\u001b[39m | \u001b[39m0.7035119\u001b[39m | \u001b[39m0.8650089\u001b[39m | \u001b[39m3.8053996\u001b[39m | \u001b[39m0.1608197\u001b[39m |\n| \u001b[39m9        \u001b[39m | \u001b[39m0.5212609\u001b[39m | \u001b[39m6.8269719\u001b[39m | \u001b[39m0.9242722\u001b[39m | \u001b[39m0.9878338\u001b[39m | \u001b[39m0.9100531\u001b[39m | \u001b[39m9.4554904\u001b[39m | \u001b[39m0.2694999\u001b[39m |\n| \u001b[39m10       \u001b[39m | \u001b[39m0.5410805\u001b[39m | \u001b[39m7.1852998\u001b[39m | \u001b[39m4.6093711\u001b[39m | \u001b[39m0.6353970\u001b[39m | \u001b[39m0.6783931\u001b[39m | \u001b[39m1.4070456\u001b[39m | \u001b[39m0.1043457\u001b[39m |\n| \u001b[39m11       \u001b[39m | \u001b[39m0.5232943\u001b[39m | \u001b[39m6.7988725\u001b[39m | \u001b[39m3.2691498\u001b[39m | \u001b[39m0.7707371\u001b[39m | \u001b[39m0.6172136\u001b[39m | \u001b[39m9.1122601\u001b[39m | \u001b[39m0.0607571\u001b[39m |\n| \u001b[39m12       \u001b[39m | \u001b[39m0.5110272\u001b[39m | \u001b[39m5.2472410\u001b[39m | \u001b[39m2.3905750\u001b[39m | \u001b[39m0.9287821\u001b[39m | \u001b[39m0.8065437\u001b[39m | \u001b[39m5.3882436\u001b[39m | \u001b[39m0.1906292\u001b[39m |\n| \u001b[39m13       \u001b[39m | \u001b[39m0.5323181\u001b[39m | \u001b[39m3.7227406\u001b[39m | \u001b[39m2.8362077\u001b[39m | \u001b[39m0.7440951\u001b[39m | \u001b[39m0.9785913\u001b[39m | \u001b[39m9.4315195\u001b[39m | \u001b[39m0.2742662\u001b[39m |\n| \u001b[39m14       \u001b[39m | \u001b[39m0.5301947\u001b[39m | \u001b[39m3.0023871\u001b[39m | \u001b[39m0.4056203\u001b[39m | \u001b[39m0.8774879\u001b[39m | \u001b[39m0.6035719\u001b[39m | \u001b[39m9.9058137\u001b[39m | \u001b[39m0.2295886\u001b[39m |\n| \u001b[39m15       \u001b[39m | \u001b[39m0.5222301\u001b[39m | \u001b[39m8.4270442\u001b[39m | \u001b[39m2.1511486\u001b[39m | \u001b[39m0.8874024\u001b[39m | \u001b[39m0.8805003\u001b[39m | \u001b[39m3.4696583\u001b[39m | \u001b[39m0.1816833\u001b[39m |\n| \u001b[39m16       \u001b[39m | \u001b[39m0.5261827\u001b[39m | \u001b[39m6.1380061\u001b[39m | \u001b[39m2.8009749\u001b[39m | \u001b[39m0.6646590\u001b[39m | \u001b[39m0.7067096\u001b[39m | \u001b[39m3.8478895\u001b[39m | \u001b[39m0.0751738\u001b[39m |\n| \u001b[39m17       \u001b[39m | \u001b[39m0.5292533\u001b[39m | \u001b[39m6.1645131\u001b[39m | \u001b[39m0.5093791\u001b[39m | \u001b[39m0.7924888\u001b[39m | \u001b[39m0.6998045\u001b[39m | \u001b[39m9.3865987\u001b[39m | \u001b[39m0.0850967\u001b[39m |\n| \u001b[39m18       \u001b[39m | \u001b[39m0.5213002\u001b[39m | \u001b[39m3.5775646\u001b[39m | \u001b[39m2.6329532\u001b[39m | \u001b[39m0.9155994\u001b[39m | \u001b[39m0.8535113\u001b[39m | \u001b[39m5.5981300\u001b[39m | \u001b[39m0.0630354\u001b[39m |\n| \u001b[39m19       \u001b[39m | \u001b[39m0.5344616\u001b[39m | \u001b[39m3.2631274\u001b[39m | \u001b[39m2.4044337\u001b[39m | \u001b[39m0.8054821\u001b[39m | \u001b[39m0.7480691\u001b[39m | \u001b[39m3.4979171\u001b[39m | \u001b[39m0.2269316\u001b[39m |\n| \u001b[39m20       \u001b[39m | \u001b[39m0.5229036\u001b[39m | \u001b[39m7.5955151\u001b[39m | \u001b[39m0.4783400\u001b[39m | \u001b[39m0.9721687\u001b[39m | \u001b[39m0.7746139\u001b[39m | \u001b[39m1.3702849\u001b[39m | \u001b[39m0.1065030\u001b[39m |\n| \u001b[39m21       \u001b[39m | \u001b[39m0.5283262\u001b[39m | \u001b[39m7.6168546\u001b[39m | \u001b[39m2.1381636\u001b[39m | \u001b[39m0.7112954\u001b[39m | \u001b[39m0.6673337\u001b[39m | \u001b[39m9.8881893\u001b[39m | \u001b[39m0.1583852\u001b[39m |\n| \u001b[39m22       \u001b[39m | \u001b[39m0.5267263\u001b[39m | \u001b[39m6.1875869\u001b[39m | \u001b[39m1.5374021\u001b[39m | \u001b[39m0.7630487\u001b[39m | \u001b[39m0.8330255\u001b[39m | \u001b[39m4.8897217\u001b[39m | \u001b[39m0.0712413\u001b[39m |\n| \u001b[39m23       \u001b[39m | \u001b[39m0.5337980\u001b[39m | \u001b[39m6.6968609\u001b[39m | \u001b[39m0.1625811\u001b[39m | \u001b[39m0.8155505\u001b[39m | \u001b[39m0.9099572\u001b[39m | \u001b[39m8.4213541\u001b[39m | \u001b[39m0.0589869\u001b[39m |\n| \u001b[39m24       \u001b[39m | \u001b[39m0.5155891\u001b[39m | \u001b[39m7.9212289\u001b[39m | \u001b[39m2.3493702\u001b[39m | \u001b[39m0.9427862\u001b[39m | \u001b[39m0.6778383\u001b[39m | \u001b[39m7.3484086\u001b[39m | \u001b[39m0.0599168\u001b[39m |\n| \u001b[39m25       \u001b[39m | \u001b[39m0.5057368\u001b[39m | \u001b[39m8.4027512\u001b[39m | \u001b[39m2.5342311\u001b[39m | \u001b[39m0.9870980\u001b[39m | \u001b[39m0.6928319\u001b[39m | \u001b[39m5.0635094\u001b[39m | \u001b[39m0.1757324\u001b[39m |\n| \u001b[39m26       \u001b[39m | \u001b[39m0.5157198\u001b[39m | \u001b[39m8.1312445\u001b[39m | \u001b[39m2.7377968\u001b[39m | \u001b[39m0.8505361\u001b[39m | \u001b[39m0.8123790\u001b[39m | \u001b[39m6.6988603\u001b[39m | \u001b[39m0.2370576\u001b[39m |\n| \u001b[39m27       \u001b[39m | \u001b[39m0.5430689\u001b[39m | \u001b[39m8.9455292\u001b[39m | \u001b[39m0.2494045\u001b[39m | \u001b[39m0.8883205\u001b[39m | \u001b[39m0.6448291\u001b[39m | \u001b[39m9.3034975\u001b[39m | \u001b[39m0.2185650\u001b[39m |\n| \u001b[39m28       \u001b[39m | \u001b[39m0.5439539\u001b[39m | \u001b[39m5.2155996\u001b[39m | \u001b[39m0.3515751\u001b[39m | \u001b[39m0.7385091\u001b[39m | \u001b[39m0.6643478\u001b[39m | \u001b[39m9.6256584\u001b[39m | \u001b[39m0.2716290\u001b[39m |\n| \u001b[39m29       \u001b[39m | \u001b[39m0.5211023\u001b[39m | \u001b[39m7.7756587\u001b[39m | \u001b[39m2.2868372\u001b[39m | \u001b[39m0.8170679\u001b[39m | \u001b[39m0.6705087\u001b[39m | \u001b[39m6.8956182\u001b[39m | \u001b[39m0.2056174\u001b[39m |\n| \u001b[39m30       \u001b[39m | \u001b[39m0.5414576\u001b[39m | \u001b[39m4.8374520\u001b[39m | \u001b[39m0.2776416\u001b[39m | \u001b[39m0.6406480\u001b[39m | \u001b[39m0.6810921\u001b[39m | \u001b[39m8.9182100\u001b[39m | \u001b[39m0.1168154\u001b[39m |\n| \u001b[39m31       \u001b[39m | \u001b[39m0.5457802\u001b[39m | \u001b[39m7.6287832\u001b[39m | \u001b[39m0.7567503\u001b[39m | \u001b[39m0.6      \u001b[39m | \u001b[39m0.9986190\u001b[39m | \u001b[39m9.8006135\u001b[39m | \u001b[39m0.2238711\u001b[39m |\n| \u001b[39m32       \u001b[39m | \u001b[39m0.4950152\u001b[39m | \u001b[39m5.0147029\u001b[39m | \u001b[39m3.6280281\u001b[39m | \u001b[39m0.9911242\u001b[39m | \u001b[39m0.6698552\u001b[39m | \u001b[39m4.8190092\u001b[39m | \u001b[39m0.2983781\u001b[39m |\n| \u001b[39m33       \u001b[39m | \u001b[39m0.5439396\u001b[39m | \u001b[39m7.8951111\u001b[39m | \u001b[39m0.3313129\u001b[39m | \u001b[39m0.6      \u001b[39m | \u001b[39m0.7121743\u001b[39m | \u001b[39m9.1335313\u001b[39m | \u001b[39m0.2241954\u001b[39m |\n| \u001b[39m34       \u001b[39m | \u001b[39m0.5361686\u001b[39m | \u001b[39m7.2963746\u001b[39m | \u001b[39m0.0152829\u001b[39m | \u001b[39m0.6      \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m9.8015984\u001b[39m | \u001b[39m0.3      \u001b[39m |\n| \u001b[35m35       \u001b[39m | \u001b[35m0.5693653\u001b[39m | \u001b[35m9.1314615\u001b[39m | \u001b[35m0.2994103\u001b[39m | \u001b[35m0.6      \u001b[39m | \u001b[35m0.6      \u001b[39m | \u001b[35m3.6569248\u001b[39m | \u001b[35m0.1537396\u001b[39m |\n=================================================================================================\n\nBest hyperparameters found:\n{'max_depth': 9, 'gamma': np.float64(0.29941037260519715), 'subsample': np.float64(0.6), 'colsample_bytree': np.float64(0.6), 'min_child_weight': 3, 'learning_rate': np.float64(0.1537396330696377), 'objective': 'binary:logistic', 'eval_metric': 'auc', 'use_label_encoder': False, 'tree_method': 'hist', 'scale_pos_weight': np.float64(1.705069124423963), 'seed': 1}\n[0]\tvalidation_0-auc:0.64414\n[20]\tvalidation_0-auc:0.68481\n[27]\tvalidation_0-auc:0.70195\n\nXGBoost Performance on Test Set:\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.74      0.74      0.74        66\n           1       0.55      0.55      0.55        38\n\n    accuracy                           0.67       104\n   macro avg       0.65      0.65      0.65       104\nweighted avg       0.67      0.67      0.67       104\n\nAccuracy: 0.6731\nROC AUC: 0.7091\nConfusion Matrix:\n[[49 17]\n [17 21]]\n\nRandom Forest Performance on Test Set:\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.72      0.76      0.74        66\n           1       0.54      0.50      0.52        38\n\n    accuracy                           0.66       104\n   macro avg       0.63      0.63      0.63       104\nweighted avg       0.66      0.66      0.66       104\n\nAccuracy: 0.6635\nROC AUC: 0.6720\nConfusion Matrix:\n[[50 16]\n [19 19]]\n\nModeling complete.\n"}],"execution_count":22},{"source":"# Results and discussion\n\nA predictive pipeline was developed and evaluated using a dataset of 691 samples with eight engineered features, which were split into 587 training and 104 test instances (~85:15 train-test split). The target variable, `cholera_severity` (1 for severe, 0 otherwise), exhibited moderate class imbalance, with approximately 37% of cases being severe, which was addressed through a calculated `scale_pos_weight` of 1.71 in the XGBoost model. Bayesian optimization identified an optimal XGBoost model configuration with a deeper tree depth of 9, moderate regularization (gamma ≈ 0.30), reduced subsampling (0.6), 60% column subsampling, a minimum child weight of 3, and a moderate learning rate of 0.15. On the test set, the tuned XGBoost model achieved an accuracy of 67%, with balanced precision and recall of approximately 74% for the non-severe class and improved sensitivity and precision for the severe class at 55%, yielding an F1-score of 0.55. The overall ROC AUC improved to 0.71, indicating a substantially better discriminatory ability than that of the previous iteration. As a baseline comparison, the Random Forest model obtained slightly lower accuracy (66%) and ROC AUC (0.67), with a similar precision but somewhat lower recall for severe cases. These results underscore the potential of machine learning models, particularly XGBoost tuned via Bayesian optimization, to predict cholera severity using epidemiological and clinical data. Nevertheless, the moderate sensitivity observed in severe cases underscores the persistent challenges in optimizing class-specific performance. This suggests the need for further feature enhancement, improved preprocessing, or alternative modeling strategies to enhance clinical applicability and patient management.","metadata":{},"cell_type":"markdown","id":"b494d1ed-791a-41ab-9dc3-0c4e0bb280a4"},{"source":"# Concluding Remarks\n\nThis notebook presents a comprehensive machine learning workflow for predicting cholera severity using individual-level epidemiological and clinical data. By incorporating thoughtful feature engineering, including key temporal and environmental factors, and applying robust modeling techniques such as Bayesian-optimized XGBoost and Random Forest, the models achieved moderate predictive performance with respective ROC AUC scores of approximately 0.67 and 0.71. While these results demonstrate the promising potential for identifying high-risk cholera cases, the sensitivity to severe outcomes remains moderate, highlighting the ongoing need for richer datasets and further methodological advancements. Future work should prioritize the integration of additional clinical indicators, exploration of alternative and ensemble algorithms, and validation across external cohorts to strengthen model robustness and generalizability. Overall, this study advances the application of data-driven approaches to enhance timely clinical decision-making and support targeted public health interventions in cholera-affected settings. If you have questions, kindly send them in at jprmaulion[at]gmail[dot]com.","metadata":{},"cell_type":"markdown","id":"3615e8fc-fdef-47c0-b929-6ca5c108af3f"},{"source":"","metadata":{"executionCancelledAt":null,"executionTime":45,"lastExecutedAt":1761916011057,"lastExecutedByKernel":"8ba82cc2-98e0-453c-8ee8-a0d4af9abef8","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":""},"cell_type":"code","id":"5a9d931f-7ce6-4af8-8ff2-15eb65530847","outputs":[],"execution_count":22}],"metadata":{"language_info":{"name":"python","version":"3.12.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (User venv)","language":"python","name":"python3"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}